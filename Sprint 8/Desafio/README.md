# Sobre o desafio
- O desafio desta sprint √© avan√ßar para a terceira etapa do desafio final, que envolve o processamento da Camada Trusted. Nesta fase, o objetivo √© assegurar que os dados sejam limpos e confi√°veis, como resultado do tratamento dos dados presentes na Raw Zone.
    
## üèÜ Tema Desafio final

#### Como tema do desafio final, escolhi analisar os filmes dos g√™neros crime e guerra lan√ßados entre 2012 e 2022. O foco da an√°lise √© entender a rela√ß√£o entre a m√©dia de avalia√ß√£o, popularidade e or√ßamento, a avalia√ß√£o de filmes com atores conhecidos e as tend√™ncias de or√ßamento e popularidade.
#### Busco responder as seguintes quest√µes: 
- Qual √© a m√©dia de avalia√ß√£o e a m√©dia de popularidade dos 10 filmes com os maiores or√ßamentos lan√ßados entre 2012 e 2022 para cada um dos g√™neros guerra e crime? Os filmes de guerra e crime analisados t√™m uma aceita√ß√£o geral positiva de acordo com a m√©dia de popularidade para seus respectivos g√™neros?  
- Qual a m√©dia de avalia√ß√£o dos filmes lan√ßados entre 2012 e 2022 para cada um dos g√™neros guerra e crime que tiveram atores com mais de 3 t√≠tulos mais conhecidos?
- A m√©dia dos or√ßamentos para filmes dos g√™neros 'guerra' e 'crime' aumentou de 2012 a 2022? E a m√©dia de popularidade desses filmes seguiu uma tend√™ncia similar durante o mesmo per√≠odo?

- Para responder √†s minhas perguntas, obtive os seguintes dados da API TMDB durante a etapa de ingest√£o:
    - Or√ßamento;
    - Popularidade no tmdb;
    - M√©dia de votos do TMDB;
    - Contagem de votos;
    - 5 atores principais;
    - Diretores;
    - G√™nero ids;
    - Data de lan√ßamento;
    - Pa√≠s de origem;
- Os dados acima vinheram na seguinte formata√ß√£o json:
![](Evidencias/print_arquivo_json_parte_1.png)
![](Evidencias/print_arquivo_json_parte_2.png)
  
## üìã Etapas

### 1Ô∏è‚É£ Decidir quais dados iria utilizar para minha an√°lise

- A primeira etapa foi analisar todos os meus dados contidos na camada Raw Zone e verificar quais dados eu iria utilizar para realizar a minha an√°lise. Ap√≥s isso, percebi que nesta etapa precisaria descartar alguns dados pois n√£o teria utilidade no foco da an√°lise das minhas quest√µes.
- Entre eles est√£o: 
    - Ano de nascimento e falecimento dos atores que est√£o no arquivo csv. 
    - Atores e diretores do json (Utilizarei apenas os dados de atores contidos no csv, mais precisamente da coluna `t√≠tulos mais conhecidos`).
    - Linguagem original. 
    - M√™s e dia de lan√ßamento.
- Por fim, para a minha an√°lise ser√° necess√°rio apenas os dados:
  - **CSV**:
    - id;
    - Titulo principal;
    - Titulo Original;
    - Ano lan√ßamento;
    - Tempo minutos;
    - G√™nero;
    - Nota media;
    - Numero de votos;
    - Genero Artista;
    - Personagem;
    - Nome do artista;
    - Profiss√£o;
    - Titulos do artista mais conhecidos.
  - **TMDB**:
    - Id;
    - Titulo;
    - Or√ßamento;
    - Popularidade;
    - Nota media;
    - Numero de votos;
    - Poster link;
    - Pais de origem;
    - G√™nero;
    - Ano de Lan√ßamento.

### 2Ô∏è‚É£ Criar o job para processamento dos arquivos json

- Ap√≥s analisar quais dados iria trabalhar, o segundo passo foi criar o job respons√°vel por realizar o processamento de padroniza√ß√£o dos dados utilizando a ferramenta do AWS Glue que √© o job.
- De in√≠cio como iria ter acesso a outra ferramentas da AWS, criei uma IAM role com as permiss√µes:
  - AmazonS3FullAcess;
  - AWSGlueConsoleFullAcess;
  - AWSLakeFormationAdmin;
  - CloudWatchFullAcess.
![](Evidencias/print_IAM_role.png)

- Logo ap√≥s, criei o job com as seguintes configura√ß√µes:
![](Evidencias/print_job_configuracao_part_1.png)
![](Evidencias/print_job_configuracao_part_2.png)
- Nos par√¢metros, adicionei `S3_INPUT_PATH` com o caminho da raw zone onde est√£o os meus arquivos json, que est√£o no caminho `s3://desafio-final-filmes-e-series-anderson-neves/Raw/TMDB/JSON/Movies/`
  ![](Evidencias/print_arquivos_json_no_bucket.png)
- Adicionei tamb√©m aos par√¢metros `S3_TARGET_PATH` o caminho alvo onde vou exportar os meus arquivos formatados em tipos parquet, que v√£o ficar na camada Trusted: `s3://desafio-final-filmes-e-series-anderson-neves/Trusted/TMDB`
![](Evidencias/print_parametros_job_json.png)
- Ap√≥s configurar o job, criei o script em spark para modelar os meus dados
- Primeiramente, o c√≥digo mostra a importa√ß√£o das bibliotecas que irei utiliza
    ```python
    import sys
    from awsglue.transforms import *
    from datetime import datetime
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from pyspark.sql.functions import col, concat_ws, round, year, trim
    from pyspark.sql.types import DoubleType, IntegerType, DecimalType
    ```
- Extrai os par√¢metros que vou trabalhar e inicializei todos os contextos de trabalho com spark e o glue (configura√ß√µes padr√µes).
    ```python
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    ```
- Defini uma vari√°vel para armazenar os caminho onde meus arquivos json est√£o, o diret√≥rio datado referece a √∫ltima ingest√£o de dados realizada pela minha fun√ß√£o lambda programada para compilar uma vez na semana, dessa forma obtenho dados atualizados.
    ```python
    source_file = args['S3_INPUT_PATH'] + '/2024/08/03/*.json'
    ```
- Recebo a data atual no formato `AAAA/MM/DD` para ser usada como particionamento dos meus dados que v√£o ser exportados para a camada trusted
  ```python
    current_date = datetime.now().strftime('%Y/%m/%d')
    target_path = f"{args['S3_TARGET_PATH']}/{current_date}/"
  ```

- Realizando a leitura e transformando meus dados json em data frame, tive que utilizar a op√ß√£o multline pois meus dados no json continham m√∫ltiplas linhas.
  ```python
  df_movies = spark.read.option('multiline', 'true').json(source_file)
  ```
- Imprimo o esquema do dataFrame antes da formata√ß√£o.
  ```python
    df_movies.printSchema()
  ```
- Seleciono apenas as colunas que vou trabalhar, descartando as outras.
  ```python
  df_movies = df_movies.select("imdb_id", "title","release_date","budget", "popularity", "vote_average", "vote_count", "poster_path" , "origin_country", "genre_ids")
  ```
- No meu json os itens pa√≠s de origem(origin_country) e g√™nero id (genre_ids) estavam em um array, para padronizar toda coluna resolvi adicionar os seus itens na mesma linha separados por v√≠gula, utilizando a fun√ß√£o `concat_ws()`
  ```python
  df_movies = df_movies.withColumn("origin_country", concat_ws(",", col("origin_country")))\
                       .withColumn("genre_ids", concat_ws(",", col("genre_ids")))
  ```
- A m√©dia de votos estava vindo em alguns casos com mais de tr√™s casas decimais, ent√£o padronizei toda a coluna para apenas uma casa decimal utilizando a fun√ß√£o `round()`.
    ```python
        df_movies = df_movies.withColumn("vote_average", round(col("vote_average"), 1))
    ```
- Como na minha an√°lise irei trabalhar apenas com o ano de lan√ßamento, extra√≠ da colunas de data de lan√ßamento apenas o ano com a fun√ß√£o `year()`
  ```python
  df_movies = df_movies.withColumn("release_year", year(col("release_date"))).drop("release_date")
  ```
- Renomeie todas as colunas para padronizar em portugu√™s
    ```python
    df_movies = df_movies.withColumnRenamed("title", "titulo") \
                                            .withColumnRenamed("budget", "orcamento") \
                                            .withColumnRenamed("poster_path", "posterLink") \
                                            .withColumnRenamed("origin_country", "paisOrigem") \
                                            .withColumnRenamed("genre_ids", "generoTMDB") \
                                            .withColumnRenamed("release_year", "anoLancamentoTMDB") \
                                            .withColumnRenamed("vote_average", "notaMediaTMDB")\
                                            .withColumnRenamed("vote_count", "numeroVotosTMDB")\
                                            .withColumnRenamed("popularity", "popularidadeTMDB")\
                                            .withColumnRenamed("imdb_id", "id")
    ```
- As colunas que continham dados que posteriomente irei realizar opera√ß√µes matem√°ticas, converti para tipos n√∫mericos utilizando a fun√ß√£o `.cast()`. Ao transformar a coluna or√ßamento para o tipo Double percebi que os dados ficavam em nota√ß√£o cient√≠fica, e como quero dados leg√≠veis nas minhas opera√ß√µes de consulta, tipei para Decimal com possiveis 38 casas de valor inteiro e 2 casas decimais(pois estamos falando de valores monet√°rios). As colunas de popularidade e m√©dia de votos n√£o tive esse problema, ent√£o deixei como double mesmo.
    ```python
    df_movies = df_movies.withColumn("orcamento", col("orcamento").cast(DecimalType(38, 2)))\
                        .withColumn("numeroVotosTMDB", df_movies["numeroVotosTMDB"].cast(IntegerType()))\
                        .withColumn("popularidadeTMDB", df_movies["popularidadeTMDB"].cast(DoubleType()))\
                        .withColumn("notaMediaTMDB", df_movies["notaMediaTMDB"].cast(DoubleType()))
    ```

- Removi poss√≠veis linhas das colunas principais com valores nulos e linhas duplicadas.
  ```python
  df_movies = df_movies.dropna(subset=['id','orcamento', 'numeroVotosTMDB', 'notaMediaTMDB', 'popularidadeTMDB'])

  df_movies = df_movies.dropDuplicates()
  ```
- Filtrei apenas os filmes que n√£o continham um id do IMDB(no meu caso s√≥ foram 3 filmes) pois n√£o iria conseguir realizar a jun√ß√£o porteriomente com o arquivo csv, pois esses filmes n√£o tinha nem o nome no arquivo csv.
    ```python
    df_movies = df_movies.filter((trim(col('id')) != "") & (col('id').isNotNull()))
    ```
- Imprimo a quantidade de linhas, esquema e primeiras linhas do DataFrame que ser√° salvo
    ```python
    print(f"quantidade de linhas: {df_movies.count()}")
    df_movies.printSchema()
    df_movies.show(truncate=False)
    ```
- Por fim, exporto o dataFrame para o tipo parquet na camada trusted no diret√≥rio TMDB, particionado pela data de processamento do job.
    ```python
    df_movies.write.mode("overwrite").parquet(target_path)

    job.commit()
    ```
- ***C√≥digo completo em: [Codigos/job_desafio_final_tmdb.py](Codigos/job_desafio_final_tmdb.py)***
- Evidencias do script:
![](Evidencias/print_job_json_script_parte_1.png)
![](Evidencias/print_job_json_script_parte_2.png)

- Evid√™ncias de execu√ß√£o:
![](Evidencias/print_evidencia_execucao_job_json_run_details.png)
- Evid√™ncia de output log
  - Na primeira parte est√° o esquema original do data frame sem o tratamento e a quantidade de linhas ap√≥s o tratamento.
  ![](Evidencias/print_evidencia_de_output_log_do_job_json_parte_1.png)
  - Na segunda parte tem o esquema de como ficou o data frame ap√≥s o tratamento e a impress√£o de algumas linhas.
  ![](Evidencias/print_evidencia_de_output_log_do_job_json_parte_2.png)



### 3Ô∏è‚É£ Criando job para processamento do arquivo csv
- Realizei a cria√ß√£o do job nos servi√ßo AWS Glue para o processamento do csv com as mesmas configura√ß√µes do job de processamento dos arquivos json.
![](Evidencias/print_configuracoes_job_csv.png)
- Como par√¢metros, passei os seguintes:
  - `S3_INPUT_PATH` referenciando o arquivo cvs de filmes no caminho `s3://desafio-final-filmes-e-series-anderson-neves/Raw/File/CSV/Movies/2024/7/18/`
    ![](Evidencias/print_arquivo_csv_no_bucket.png)
  - `S3_REFERENCE_DF_PATH` referenciando o arquivo parquet salvo no √∫ltimo job no caminho: `s3://desafio-final-filmes-e-series-anderson-neves/Trusted/TMDB/`
  - `S3_TARGET_PATH` referencia o destino que ser√° exportado o data frame do job na camada trusted: `s3://desafio-final-filmes-e-series-anderson-neves/Trusted/CSV`
  - Evid√™ncia:
    ![](Evidencias/print_parametros_job_csv.png)
- Ap√≥s configurar o job realizei a cria√ß√£o do script para o processamento dos dados
- Primeiramente, no script s√£o importado as bibliotecas que ser√£o utilizadas: 
  ```python
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from pyspark.sql.types import IntegerType, DoubleType
  ```
- Extrai os par√¢metros que vou trabalhar e inicializei todas os contextos de trabalho com spark e o glue (configura√ß√µes padr√µes).
    ```python
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH','S3_TARGET_PATH','S3_REFERENCE_DF_PATH'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    ```
- Na vari√°vel source_file defino o caminho em que o arquivo csv de filmes se encontra e target_path defino o caminho onde o data frame ser√° exportado.
    ```python
    source_file = args['S3_INPUT_PATH']
    target_path = args['S3_TARGET_PATH']
    ```
- A vari√°vel df_path_to_filter_reference referencia o caminho onde est√° o meus arquivos parquet do job anterior na parti√ß√£o que se encontra.
  ```python
  df_path_to_filter_reference = args['S3_REFERENCE_DF_PATH'] + '2024/08/11/'
  ```
- Realizo a leitura do arquivo csv transformando em dataFrame e removo as colunas que n√£o irei utilizar.
  ```python
  df_movies_csv = spark.read.csv(source_file, header=True, inferSchema=True, sep="|")

  df_movies_csv = df_movies_csv.drop("anoNascimento", "anoFalecimento")
  ```
- Realizo a leitura dos arquivos parquets transformando em dataFrame e imprimo a quantidade de linhas, esquema e as primeiras linhas.
    ```python
    data_frame_refence = spark.read.parquet(df_path_to_filter_reference)
    print(f"quantidade de linhas: {data_frame_refence.count()}")
    data_frame_refence.printSchema()
    data_frame_refence.show(truncate=False)
    ```
- Seleciono apenas a coluna id do dataFrame que cont√©m os filmes de guerra e crime de 2012 a 2022 extra√≠dos da api TMDB que cont√©m os ids dos filmes do IMDB 
    ```python
    ids_IMDB_parquet = data_frame_refence.select("id").distinct()
    ```
- Realizo uma jun√ß√£o do dataFrame com os dados dos filmes contidos no csv com apenas os ids dos filmes que tenho os dados extra√≠dos do tmdb. Motivos pelos quais realizei essa filtragem:
  -  Alguns filmes do g√™nero crime e guerra n√£o s√£o considerados esse g√™nero no IMDB (origem dos dados do csv) e no TMDB eles s√£o considerados, dessa forma tenho mais filmes a serem analisados.
     -  Por exemplo, o filme Dose dupla n√£o √© considerado filme de crime no IMDB e no TMDB ele √©
     ![](Evidencias/print_filme_dose_dupla_imdb.png)
     ![](Evidencias/print_filme_dose_dupla_tmdb.png)
  - Outro motivo e o principal, √© que os filmes provindos do json tem os dados que quero para minha an√°lise (or√ßamento e popularidade) e no csv n√£o tenho essas informa√ß√µes. Dessa forma, consigo filtrar somente os dados relevantes para a minha an√°lise.
- Realizei essa jun√ß√£o atrav√©s do m√©todo `.join` passando como par√¢metro os ids dos filmes do dataFrame com dados provindos do TMDB, referenciando a jun√ß√£o pela coluna id do dataFRame do csv e a jun√ß√£o ser√° da forma inner, ou seja, apenas os ids em comum.
    ```python
    df_filtered_movies_csv = df_movies_csv.join(ids_IMDB_parquet, on="id", how="inner")
    ```
- Converti as colunas que posteriomete vou realizar c√°lculos matem√°ticos para tipos num√©ricos correspondentes.
    ```python
    df_filtered_movies_csv = df_filtered_movies_csv.withColumn("anoLancamento", df_filtered_movies_csv["anoLancamento"].cast(IntegerType()))\
                                            .withColumn("notaMedia", df_filtered_movies_csv["notaMedia"].cast(DoubleType()))\
                                            .withColumn("numeroVotos", df_filtered_movies_csv["numeroVotos"].cast(IntegerType()))
    ```
- Renomeei a coluna de t√≠tulo principal pois estava com o nome incorreto
    ```python
    df_filtered_movies_csv = df_filtered_movies_csv.withColumnRenamed("tituloPincipal", "tituloPrincipal")
    ```
- Removendo poss√≠veis linhas duplicadas
    ```python
    df_filtered_movies_csv= df_filtered_movies_csv.dropDuplicates()
    ```
- Imprimindo o resultado final do dataFrame a quantidade de linhas, o esquema que ficou e as primeiras linhas.
    ```python
    print(f"quantidade de linhas: {df_filtered_movies_csv.count()}")
    df_filtered_movies_csv.printSchema()
    df_filtered_movies_csv.show(truncate=False)
    ```
- Por fim, exporto o dataFrame com os dados do csv em arquivos do tipo parquet para dentro do diret√≥rio CSV na camada Trusted do bucket do meu desafio final.
    ```python
    df_filtered_movies_csv.write.mode("overwrite").parquet(target_path)

    job.commit()
    ```
- ***C√≥digo completo em: [Codigos/job_desafio_final_csv.py](Codigos/job_desafio_final_csv.py)***
- Evid√™ncia de execu√ß√£o do job:
![](Evidencias/print_evidencia_execucao_job_csv_details.png)
- Evid√™ncias output log no CloudWatch:
  - Na primeira parte temos a quantidade de linhas, esquema e primeiras linhas do dataFrame com os dados do TMDB:
  ![](Evidencias/print_evidencia_de_output_log_do_csv_json_parte_1.png)
  - Nessa segunda parte temos a quantidade de linhas, o esquema e algumas das primeiras linhas do dataFrame com os dados tratados do arquivo csv de filmes:
  ![](Evidencias/print_evidencia_de_output_log_do_csv_json_parte_2.png)

### 4Ô∏è‚É£ Evid√™ncias da camada Trusted ap√≥s execu√ß√£o dos jobs
![](Evidencias/print_evidencia_camada_trusted_no_bucketS3.png)
![](Evidencias/print_evidencia_camada_trusted_no_bucketS3_parte_.png)
![](Evidencias/print_evidencia_camada_trusted_particao_csv.png)
![](Evidencias/print_evidencia_camada_trusted_particao_TMDB.png)

### 5Ô∏è‚É£ Cria√ß√£o do data base
- Ap√≥s a limpeza e padroniza√ß√£o dos meus dados, criei um database nos servi√ßos da AWS glue
![](Evidencias/print_DataBase_criado.png)

### 6Ô∏è‚É£ Cria√ß√£o do Crawler
- Com o data base criado o pr√≥ximo passo foi criar o crawler respons√°vel por catalogar os meus dados da camada Trusted e criar as tabelas do meu data base a partir dos meus dados padronizados em parquet.
- Crawler criado com as seguintes configura√ß√µes:
  - Dados de origem est√£o apontando para o bucket s3 na camada trusted
  - Define a IAM role AWSGlueServiceRole-DesafioFinal-Etapa3 com as pol√≠ticas evidenciadas acima.
  - O data base de destino √© de desafio-final-filmes evidenciado acima, agendado sob demanda.
![](Evidencias/print_configuracoes_crawler.png)

- Ap√≥s a cria√ß√£o do crawler, foi executado o crawler cria√ß√£o das tabelas.
- Evid√™ncias de execu√ß√£o do crawler:
![](Evidencias/print_evidencia_execucao_crawler.png)
- Com a execu√ß√£o do crawler foram criadas duas tabelas CSV e TMDB com as parti√ß√µes de data de processamento dos dados.
![](Evidencias/print_evidencia_execucao_crawler_parte_2.png)
- Evid√™ncia das tabelas criadas no data base:
![](Evidencias/print_evidencia_tabelas_criadas_com_execucao_do_crawler.png)
### 7Ô∏è‚É£ Executando consultas no AWS Athena
- Com as tabelas criadas, com base nos dados da camada trusted, realizei consultas em SQL utilizando o servi√ßo da AWS Athena.
- Primeiramente, tive que selecionar o banco de dados desafio-final-filmes e carregar as parti√ß√µes da tabela TMDB.
![](Evidencias/print_evidencia_athena_carregando_particoes.png)
- Consultando todos os dados da tabela TMDB
```sql
select * from tmdb;
```
- Como resultado, obteve um total de 860 linhas, com a mesma quantidade do dataframe importado.
![](Evidencias/print_evidencia_athena_consultando_todos_os_dados_tmdb.png)
- Exibindo todos os dados da tabela CSV
```sql
select * from csv;
```
- Como resultado, obteve um total de 3378 linhas, com a mesma quantidade do dataframe importado.
![](Evidencias/print_evidencia_athena_consultando_todos_os_dados_csv.png)
- Consultando se h√° dados duplicados no tmdb
    ```sql
    SELECT t.*
    FROM tmdb t
    JOIN (
        SELECT id
        FROM tmdb
        GROUP BY id
        HAVING COUNT(id) > 1
    ) duplicadas ON t.id = duplicadas.id
    ORDER BY t.id
    ```
    ![](Evidencias/print_evidencia_athena_consultando_ids_duplicados_tmdb.png)
- Realizando jun√ß√£o das duas tabelas
    ```sql
    select * from tmdb join csv on tmdb.id = csv.id;
    ```
    ![](Evidencias/print_evidencia_athena_realizando_juncao_nas_duas_tabelas_parte_1.png)
    ![](Evidencias/print_evidencia_athena_realizando_juncao_nas_duas_tabelas_parte_2.png)
    ![](Evidencias/print_evidencia_athena_realizando_juncao_nas_duas_tabelas_parte_3.png)
    ![](Evidencias/print_evidencia_athena_realizando_juncao_nas_duas_tabelas_parte_4.png)
- Somando toda a coluna or√ßamento
    ```sql
    select sum(orcamento) orcamento_total count(*) quantidade_linhas from tmdb;
    ```
    ![](Evidencias/print_evidencia_athena_soma_da_coluna_orcamento.png)

- ***Todas as consultas em sql: [Codigos/consultas.sql](Codigos/consultas.sql)***

### Ap√≥s analisar os dados da camada Trusted e ver que est√£o padronizados e s√£o confi√°veis, os dados est√£o prontos para ir para pr√≥xima etapa do desafio final.

### Refer√™ncias

- [Documenta√ß√£o de fun√ß√µes Spark Sql](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)

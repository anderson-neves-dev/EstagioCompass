# Sobre o desafio
- O desafio desta sprint consiste na entrega da quarta etapa do desafio final, que envolve o processamento da Camada Refined. Nesta fase, o objetivo √© assegurar que os dados sejam confi√°veis e realizar a modelagem multidimensional dos dados contidos na camada trusted da etapa 3 do desafio.
    
## üèÜ Tema Desafio final

#### Como tema do desafio final, escolhi analisar os filmes dos g√™neros crime e guerra lan√ßados entre 2012 e 2022. O foco da an√°lise √© entender a rela√ß√£o entre a m√©dia de avalia√ß√£o, popularidade e or√ßamento, os generos que mais combinam e as tend√™ncias de or√ßamento e popularidade.

#### Busco responder as seguintes quest√µes:

- Qual √© a m√©dia de avalia√ß√£o e a m√©dia de popularidade dos 10 filmes com os maiores or√ßamentos lan√ßados entre 2012 e 2022 para cada um dos g√™neros guerra e crime? Os filmes de guerra e crime analisados t√™m uma aceita√ß√£o geral positiva de acordo com a m√©dia de popularidade para seus respectivos g√™neros?

- Quais os g√™neros que mais "combinam" com os g√™neros de crime e guerra? 

- Quais os paises que mais produziram filmes desses generos diante esse intervalo de tempo?

- A m√©dia dos or√ßamentos para filmes dos g√™neros 'guerra' e 'crime' aumentou de 2012 a 2022? E a m√©dia de popularidade desses filmes seguiu uma tend√™ncia similar durante o mesmo per√≠odo?
    
## üìã Etapas

### 1Ô∏è‚É£ Criar o modelo dimensional para os dados da camada trusted

- A primeira etapa foi analisar todos os meus dados contidos na camada Trusted Zone e gerar o modelo dimensional dos dados com as tabelas dimens√µes e a tabela fato. 
- Meus dados inicialmente na camada trusted estavam da seguinte forma: 
![](Evidencias/esquema_dos_dados_na_camada_trusted.jpeg)
- Evid√™ncia dos dados na camada trusted: 
![](Evidencias/print_envidencia_camada_trusted_dados_csv.png)
![](Evidencias/print_envidencia_camada_trusted_dados_tmdb.png)
- Evid√™ncia de consultas dos dados da camada trusted no Athena:
![](Evidencias/print_evidencia_consulta_athena_dados_camada_trusted_tmdb.png)
![](Evidencias/print_evidencia_consulta_athena_dados_camada_trusted_csv.png)

- Ap√≥s analisar os dados na camada trusted e relacionar como eles iriam responder as minhas quest√µes, criei o seguinte modelo dimensional:
![](Evidencias/modelo_dimensional.jpeg)
- Ent√£o, vou estar criando as seguintes tabelas com os atributos:
  - Dimens√£o filme:
    - id_filme;
    - T√≠tulo principal;
    - T√≠tulo original;
    - Ano de lan√ßamento;
    - Poster link.
  - Dimens√£o pa√≠s:
    - id_pais;
    - S√≠gla dos pa√≠ses.
  - Dimens√£o g√™nero:
    - id_genero;
    - Nome dos g√™neros.
  - Fato filme:
    - id_filme;
    - id_genero;
    - id_pais;
    - Or√ßamento;
    - Popularidade do tmdb;
    - Nota m√©dia do tmdb;
    - Nota m√©dia do imdb;
    - N√∫mero de votos tmdb;
    - N√∫mero de votos imdb;   


### 2Ô∏è‚É£ Criar o job para processamento da camada Refined

- Ap√≥s definir o esquema dimensional que iria utilizar, o segundo passo foi criar o job respons√°vel por realizar o processamento de modelagem desses dados utilizando spark no ETL job dos servi√ßos AWS Glue.
- Configura√ß√µes do job:
![](Evidencias/print_configuracao_job_parte_1.png)
- Adicionei uma IAM role que foi criada na terceira etapa para conseguir utilizar outros servi√ßos da AWS. As pol√≠ticas foram:
    - AmazonS3FullAcess;
    - AWSGlueConsoleFullAcess;
    - AWSLakeFormationAdmin;
    - CloudWatchFullAcess.
![](Evidencias/print_IAM_role.png)
![](Evidencias/print_configuracao_job_parte_2.png)

- Nos par√¢metros, adicionei `S3_INPUT_CSV_PATH` com o caminho da camada trusted no bucket onde est√£o os arquivos parquet provindos do CSV, que est√£o no caminho: `s3://desafio-final-filmes-e-series-anderson-neves/Trusted/CSV/` e `S3_INPUT_TMDB_PATH` com os arquivos parquet com os dados provindos da api TMDB que est√£o no caminho: `s3://desafio-final-filmes-e-series-anderson-neves/Trusted/TMDB`

- Adicionei tamb√©m aos par√¢metros `S3_TARGET_PATH` com o caminho alvo onde vou exportar os meus dataframes dimensionados em arquivos parquet, que v√£o ficar na camada Refined: `s3://desafio-final-filmes-e-series-anderson-neves/Refined`
![](Evidencias/print_configuracao_job_parte_3.png)

- Ap√≥s configurar o job, criei o script em spark para a modelagem multidimensional dos dados.
- Primeiramente, o c√≥digo cont√©m a importa√ß√£o das bibliotecas que irei utilizar
    ```python
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from pyspark.sql.functions import split, explode, col, monotonically_increasing_id, concat, lit
    from pyspark.sql.types import IntegerType
    ```
- Extrai os par√¢metros que vou trabalhar e inicializei todos os contextos de trabalho com spark e o glue (configura√ß√µes padr√µes).
    ```python
    args = getResolvedOptions(sys.argv, ['JOB_NAME', args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_CSV_PATH','S3_INPUT_TMDB_PATH', 'S3_TARGET_PATH'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    ```
- Defini uma vari√°vel para armazenar os caminhos onde meus arquivos parquet est√£o, o diret√≥rio datado referecia o √∫ltimo tratamento realizado na camada trusted.
    ```python
    S3_INPUT_CSV_PATH = args['S3_INPUT_CSV_PATH']
    S3_INPUT_TMDB_PATH = args['S3_INPUT_TMDB_PATH'] + '/2024/08/11/' 
    ```

- Realizando a leitura e transformando meus arquivos parquet em dataframes.
    ```python
    df_csv = spark.read.parquet(S3_INPUT_CSV_PATH)
    df_tmdb = spark.read.parquet(S3_INPUT_TMDB_PATH)
    ```
- Imprimo o esquema do dataFrame antes da formata√ß√£o.
    ```python
    df_csv.printSchema()
    df_tmdb.printSchema()
    ```
- Comecei a modelagem dos dados pela dimens√£o pa√≠s. 
- Primeiramente selecionei apenas a coluna `paisOrigem` dos dados do tmdb
    ```python
    df_pais = df_tmdb.select("paisOrigem")
    ```
- Com os dados de pa√≠s, transformei a coluna 'paisOrigem' em m√∫ltiplas linhas, uma para cada pais, pois alguns filmes s√£o internacionais e os pa√≠ses est√£o agrupados na mesma linha separados por v√≠rgula, assim como os de g√™nero. Para isso utilizei a fun√ß√£o `explode()` em conjunto com `split()` para "explodir" as linhas com mais de um valor separados por v√≠rgula.
    ```python
    df_pais = df_pais.withColumn("paisOrigem", explode(split(col("paisOrigem"), ",")))
    ```
- Como os dados de pa√≠s e g√™nero estavam agrupados:
![](Evidencias/print_dados_pais_e_genero_agrupados_como_lista.png)
- Removendo linhas duplicadas para conter apenas um pa√≠s por linha e removendo linhas vazias que n√£o possuem pa√≠ses.
    ```python
    df_pais = df_pais.dropDuplicates()
    df_pais = df_pais.filter(col("paisOrigem") != "")
    ```

- Adicionei uma coluna id_pais com um id para cada pa√≠s utilizando a fun√ß√£o `monotonically_increasing_id()`.
    ```python
    df_pais = df_pais.withColumn("id_pais", monotonically_increasing_id())
    ```
- Transformei a coluna 'paisOrigem' em m√∫ltiplas linhas, uma para cada pais no data frame com os dados tmdb, adicionando esses dados em um novo dataframe geral chamando df_filme que vou utilizar para juntar todos os dados ao decorrer da transforma√ß√£o dos dados.
    ```python
    df_filme = df_tmdb.withColumn("paisOrigem", explode(split(col("paisOrigem"), ",")))
    ```
- Realizo a jun√ß√£o do dataframe de df_filme com o df_pais atrav√©s da coluna paisOrigem. Realizei essa jun√ß√£o com o m√©todo `inner` pois dessa forma filtro apenas os dados que tenho os pa√≠ses de origem dos filmes que s√£o dados relevantes para minha an√°lise (apenas 5 filmes foram descartados).
    ```python
    df_filme = df_filme.join(df_pais, on="paisOrigem", how="inner")
    ```
- Renomenado a coluna paisOrigem
    ```python
    df_pais = df_pais.withColumnRenamed("paisOrigem", "pais")
    ```
- A pr√≥xima etapa do script foi criar um dataframe para armazenar as informa√ß√µes sobre os g√™neros, que incluem o nome e o ID. A coluna de g√™neros resgatada da API TMDB na etapa de ingest√£o de dados cont√©m apenas os [IDs dos g√™neros](Evidencias/print_dados_pais_e_genero_agrupados_como_lista.png). Esses dados foram extra√≠dos diretamente da documenta√ß√£o da API TMDB. Como esses valores s√£o fixos e h√° apenas 19 itens, para padronizar, decidi traduzi-los para o portugu√™s.
- Obs.: Estou trabalhando com base nos g√™neros da API TMDB pois muitos filmes dos arquivos CSV n√£o s√£o considerados do g√™nero de crime e guerra e no TMDB s√£o. Evid√™ncio um exemplo disso na entrega da [etapa 3 na sprint 8](/Sprint%208/Desafio)
    ```python
    generos_rdd = spark.sparkContext.parallelize([\
        (28, "A√ß√£o"),\
        (12, "Aventura"),\
        (16, "Anima√ß√£o"),\
        (35, "Com√©dia"),\
        (80, "Crime"),\
        (99, "Document√°rio"),\
        (18, "Drama"),\
        (10751, "Fam√≠lia"),\
        (14, "Fantasia"),\
        (36, "Hist√≥ria"),\
        (27, "Terror"),\
        (10402, "M√∫sica"),\
        (9648, "Mist√©rio"),\
        (10749, "Romance"),\
        (878, "Fic√ß√£o Cient√≠fica"),\
        (10770, "Filme para TV"),\
        (53, "Suspense"),\
        (10752, "Guerra"),\
        (37, "Faroeste")\
    ])

    # Criando dataFrame de generos apartir do rdd
    df_generos = spark.createDataFrame(generos_rdd, ["id_genero", "genero"])
    ```
- Evid√™ncia da extra√ß√£o dos dados de g√™nero na documenta√ß√£o da API TMDB:
![](Evidencias/print_evidencia_resgatando_dados_de_generos_de_filmes_na_api_TMDB.png)

- Novamente usei a fun√ß√£o ¬¥explode()¬¥ para dessa vez transformar a coluna 'generoTMDB' do df_filmes em m√∫ltiplas linhas, uma para cada g√™nero e ap√≥s isso realizo a jun√ß√£o com o `df_genero`.
    ```python
    df_filme = df_filme.withColumn("id_genero", explode(split(col("generoTMDB"), ",")))

    df_filme = df_filme.join(df_generos, on="id_genero", how="inner")
    ```

- Ap√≥s isso, selecionei dos dados do CSV apenas as colunas: id, tituloPrincipal, tituloOriginal, notaMedia e numeroVotos. As duas √∫ltimas colunas cont√©m dados provindos da plataforma de an√°lise de filmes IMDB. Depois elimino as linhas duplicadas e realizo a jun√ß√£o com o dataframe geral dos dados.
    ```python
    df_csv_filtrado = df_csv.select("id", "tituloPrincipal", "tituloOriginal", "notaMedia", "numeroVotos")

    df_csv_filtrado = df_csv_filtrado.dropDuplicates()

    df_filme = df_filme.join(df_csv_filtrado, on="id", how="inner")
    ```
- Com todos os dados j√° salvos, filtrados e juntados, o pr√≥ximo passo foi padroniz√°-los.
- Primeiramente, renomeando os nomes das colunas.
    ```python
    df_filme = df_filme.withColumnRenamed("id", "id_filme")\
                        .withColumnRenamed("tituloOriginal", "titulo_original")\
                        .withColumnRenamed("tituloPrincipal", "titulo_principal")\
                        .withColumnRenamed("anoLancamentoTMDB", "ano_lancamento")\
                        .withColumnRenamed("posterLink", "poster_link")\
                        .withColumnRenamed("notaMediaTMDB", "nota_media_TMDB")\
                        .withColumnRenamed("numeroVotosTMDB", "numero_votos_TMDB")\
                        .withColumnRenamed("notaMedia", "nota_media_IMDB")\
                        .withColumnRenamed("numeroVotos", "numero_votos_IMDB")\
                        .withColumnRenamed("popularidadeTMDB", "popularidade_TMDB")

    ```
- Adicionando a base do link para ter acesso ao poster dos filmes, pois de padr√£o da API √© retornado apenas uma parte do link que √© expec√≠fico do filme.
    ```python
    df_filme = df_filme.withColumn('poster_link', concat(lit('https://image.tmdb.org/t/p/w500'), df_filme['poster_link']))
    ```
![](Evidencias/print_evidencia_dados_coluna_posterlink_antes.png)
- Padronizando as colunas de id_genero e id_pais para o tipo integer nos dataframes que as cont√©m.
    ```python
    df_filme = df_filme.withColumn("id_genero", col("id_genero").cast(IntegerType()))\
                    .withColumn("id_pais", col("id_pais").cast(IntegerType()))
    df_pais = df_pais.withColumn("id_pais", col("id_pais").cast(IntegerType()))
    df_generos = df_generos.withColumn("id_genero", col("id_genero").cast(IntegerType()))
    ```
- Criando o dataframe da dimens√£o filme conforme o modelo dimensional dos meus dados e eliminando linhas duplicadas.
    ```python
    dim_filme = df_filme.select("id_filme", "titulo_original", "titulo_principal", "poster_link", "ano_lancamento")
    dim_filme = dim_filme.dropDuplicates()
    ```
- Criando os dataframes das dimens√£o pa√≠s e g√™nero
    ```python
    dim_pais = df_pais
    dim_genero = df_generos
    ```
- Criando dataframe fato filmes conforme o modelo dimensional dos meus dados.
```python
fato_filme = df_filme.select("id_filme", "id_genero", "id_pais", "orcamento", "popularidade_TMDB", "nota_media_TMDB", "numero_votos_TMDB", "nota_media_IMDB", "numero_votos_IMDB")
```
- Imprimo a quantidade de linhas e o esquema de cada dataframe dimensional
print(f"Quantidade de filmes: {dim_filme.count()}")
dim_filme.printSchema()
print(f"Quantidade de paises: {dim_pais.count()}")
dim_pais.printSchema()
print(f"Quantidade de g√™neros: {dim_genero.count()}")
dim_genero.printSchema()
print(f"Quantidade de linhas da tabela fato filmes: {fato_filme.count()}")
fato_filme.printSchema()

- Por fim, salvo os dataframe dimensionais como arquivo parquet na camada Refined particionando de acordo cada dataframe.
```python
dim_filme.write.mode("overwrite").parquet(f'{S3_TARGET_PATH}/dim_filme')
dim_genero.write.mode("overwrite").parquet(f'{S3_TARGET_PATH}/dim_genero')
dim_pais.write.mode("overwrite").parquet(f'{S3_TARGET_PATH}/dim_pais')
fato_filme.write.mode("overwrite").parquet(f'{S3_TARGET_PATH}/fato_filme')

job.commit()
```

- C√≥digo completo com os devidos coment√°rios em: [Codigos/job/job_processamento_camada_refined.py](Codigos/job/job_processamento_camada_refined.py)
- Evid√™ncia do c√≥digo no ETL job:
![](Evidencias/print_evidencia_codigo_do_script_no_job_parte_1.png)
![](Evidencias/print_evidencia_codigo_do_script_no_job_parte_2.png)
![](Evidencias/print_evidencia_codigo_do_script_no_job_parte_3.png)

- Evid√™ncia do job sendo executado com sucesso:
![](Evidencias/print_evidencia_job_executado_com_sucesso.png)
- Evid√™ncia de log no ClouldWatch do job ap√≥s ser executado:
  - Esquema de como os dados estavam do csv e tmdb da camada trusted:
  ![](Evidencias/print_evidencia_de_execucao_log_cloudwatch_parte_1.png)
  - Quantidade de linhas e esquema de como ficaram os dados dimensionados:
  ![](Evidencias/print_evidencia_de_execucao_log_cloudwatch_parte_2.png)
- Evid√™ncia dos dados na camada Refined no bucket s3:
![](Evidencias/print_evidencia_bucket_com_diretorio_refined.png)
![](Evidencias/print_evidencia_bucket_camada_refined_particionamentos.png)
![](Evidencias/print_evidencia_bucket_camada_refined_dimensao_filme.png)
![](Evidencias/print_evidencia_bucket_camada_refined_dimensao_genero.png)
![](Evidencias/print_evidencia_bucket_camada_refined_dimensao_pais.png)
![](Evidencias/print_evidencia_bucket_camada_refined_fato_filme.png)

### 3Ô∏è‚É£ Cria√ß√£o do Crawler
- Com os dados armazenados na camada refined o pr√≥ximo passo foi criar o crawler respons√°vel por catalogar os meus dados da camada Refined e criar as tabelas em um database a partir dos meus dados padronizados em parquet.
- Evid√™ncia de database criado nos servi√ßos da AWS glue:
![](Evidencias/print_evidencia_database_criado.png)

- Crawler criado com as seguintes configura√ß√µes:
    - Dados de origem est√£o apontando para o bucket s3 na camada Refined
    - Defini a IAM role AWSGlueServiceRole-DesafioFinal-Etapa3 com as pol√≠ticas evidenciadas acima.
    - O database de destino √© de desafio-final-filmes-modelo-dimensional evidenciado acima, agendado sob demanda.
![](Evidencias/print_evidencia_configuracao_crawler.png)
- Ap√≥s a cria√ß√£o do crawler, foi executado o crawler cria√ß√£o das tabelas.
- Evid√™ncias de execu√ß√£o do crawler:
![](Evidencias/print_evidencia_execucao_crawler.png)
- Com a execu√ß√£o do crawler foram criadas quatro tabelas: Dimens√£o de filmes, genero, pais e a tabela fato filmes.
- Evid√™ncia das tabelas criadas no database:
![](Evidencias/print_evidencia_tabelas_criadas_no_database_apos_execucao_crawler.png)
### 4Ô∏è‚É£ Executando consultas no AWS Athena
- Com as tabelas multidimensionais criadas, com base nos dados da camada Refined, realizei consultas em SQL para testes da confiabilidade dos dados utilizando o servi√ßo da AWS Athena.
- Primeiramente, tive que selecionar o banco de dados desafio-final-filmes-modelo-dimensional.
![](Evidencias/print_evidencia_tabelas_no_athena_parte_1.png)
![](Evidencias/print_evidencia_tabelas_no_athena_parte_2.png)

- Consultando todos os dados da tabela dimens√£o de filmes. Como resultado, obtive um total de 834 filmes, com a mesma quantidade do dataframe salvo.
![](Evidencias/print_evidencia_athena_consulta_dimensao_filmes.png)
- Consultando todos os dados da tabela dimens√£o de pa√≠s. Como resultado, obtive um total de 60 pa√≠ses, com a mesma quantidade do dataframe salvo.
![](Evidencias/print_evidencia_athena_consulta_dimensao_pais.png)
- Consultando todos os dados da tabela dimens√£o de g√™nero. Como resultado, obtive um total de 19 g√™neros, com a mesma quantidade do dataframe salvo.
![](Evidencias/print_evidencia_athena_consulta_dimensao_genero.png)
- Consultando todos os dados da tabela fato filmes. Como resultado, obtive um total de 3114 linhas, com a mesma quantidade do dataframe salvo.
![](Evidencias/print_evidencia_athena_consulta_fato_filme.png)

- Realizando a jun√ß√£o de todas as dimens√µes com a tabela fato:
![](Evidencias/print_evidencia_consulta_athena_juncao_de_todas_tabelas_dimensao_com_tabela_fato_parte_1.png)
![](Evidencias/print_evidencia_consulta_athena_juncao_de_todas_tabelas_dimensao_com_tabela_fato_parte_2.png)

- Consulta de soma e m√©dia de or√ßamento e quantidade de filmes por ano. Foi realizado uma subconsulta que faz uma m√©dia dos or√ßamento da tabela fato agrupados pelo id_filme para n√£o ocorrer v√°rias somas do or√ßamento do mesmo filme, visto que a tabela fato tem linhas multivaloradas de g√™nero e pa√≠s.
![](Evidencias/print_evidencia_athena_consulta_orcamento_por_ano.png)

- Consulta da m√©dia de avalia√ß√µes do TMDB, IMDB e popularidade dos filmes de crime por g√™neros para se saber quais g√™neros mais 'combinam' com os de crime. 
![](Evidencias/print_evidencia_consulta_media_de_avaliacoes_dos_filmes_de_crime_por_generos.png)
- ***Todas as consultas maiores em sql: [Codigos/consultas_sql/](Codigos/consultas_sql/)***

### Ap√≥s analisar os dados da camada Refined e ver que est√£o padronizados, multidimensionados e s√£o confi√°veis, os dados est√£o prontos para gera√ß√£o de relat√≥rios da pr√≥xima etapa do desafio final.

### Refer√™ncias

- [Documenta√ß√£o de fun√ß√µes Spark Sql](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
